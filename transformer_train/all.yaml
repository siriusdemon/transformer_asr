data:
  name: all
  vocab: /media/data/weijiang_hd_data/djl/zh_features/all_vocab.t
  batch_size: 64
  text: character
  train: /media/data/weijiang_hd_data/djl/zh_features/all_train_10.csv
  test: /media/data/weijiang_hd_data/djl/zh_features/all_test.csv
  dev: /media/data/weijiang_hd_data/djl/zh_features/all_dev.csv
  short_first: False
  num_mel_bins: 40
  apply_cmvn: False
  normalization: False
  spec_argument: False
  left_frames: 0
  right_frames: 0
  skip_frames: 0
  from_kaldi: False
  if_augment: False
  num_works: 4
model:
  # network architecture
  type: transformer
  d_model: 256
  normalize_before: False
  concat_after: False
  # dropout
  pos_dropout_rate: 0.0
  ffn_dropout_rate: 0.0
  slf_attn_dropout_rate: 0.0
  src_attn_dropout_rate: 0.0
  residual_dropout_rate: 0.1
  # encoder related
  feat_dim: 40
  num_enc_blocks: 6
  enc_ffn_units: 1024
  enc_input_layer: conv2d
  # decoder related
  vocab_size: 5337
  num_dec_blocks: 6
  dec_ffn_units: 1024
  # attention related
  n_heads: 4
  # label smoothing
  smoothing: 0.1
  activation: glu
  share_embedding: True
train:
  scheduler: stepwise
  optimizer: adam
  warmup_steps: 25000
  shuffle: True
  lr: 1.0
  clip_grad: 5
  epochs: 200
  accum_steps: 1
  grad_noise: False
  train_model: model.train_best.pt
  model_path: model.best.pt
  save_name: transformer
  mixed_precision: False
